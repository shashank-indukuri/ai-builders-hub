# AutoGen Planner + Assistant on Gemini (Minimal Streamlit UI) ğŸ¤–

A minimal, learning-focused multiâ€‘agent workflow: a Planner proposes steps; an Assistant writes and executes Python via a User Proxy â€” all running on Gemini and wrapped in a clean Streamlit UI.


## Why This Matters

- ğŸ§  Twoâ€‘agent reasoning: A clear separation of planning vs. execution.
- ğŸ”§ Real code execution: Assistantâ€™s Python blocks run locally (via User Proxy).
- ğŸ§ª Learning-first: Simple twoâ€‘step flow without legacy function-calling.
- ğŸ”’ [.env] controls everything.


## Key Features

- **Planner + Assistant**: Planner suggests steps; Assistant implements them in code.
- **Two-Step Flow**: Get plan first â†’ pass plan to Assistant as context.
- **Local Execution**: Code runs in `planning/` workdir via the User Proxy.
- **Minimal UI**: One text area and â€œRun Multi-Agent Chatâ€ button.
- **.env Support**: Configure Gemini and runtime settings via environment variables.


## Installation and Setup

### Prerequisites
- Python 3.8 or later
- [uv](https://docs.astral.sh/uv/) (recommended) or pip
- Gemini API key
- Optional: Docker Desktop (if you want to run generated code in Docker)

### 1) Create .env with Gemini

Create a [.env] at the project root:

```bash
GEMINI_API_KEY=your_gemini_api_key_here
# Optional overrides:
GEMINI_MODEL=gemini-2.0-flash
GEMINI_BASE_URL=[https://generativelanguage.googleapis.com/v1beta/openai](https://generativelanguage.googleapis.com/v1beta/openai)

# Runtime options (optional)
TEMPERATURE=0
MAX_AUTO_REPLIES=10
USE_DOCKER=false

Tip: An .env.example is included; copy it to .env and fill your values.

### 2) Install Dependencies
Using uv (recommended):
```bash
uv sync
```

### 3) Run the App
```bash
uv run streamlit run streamlit_app.py
```

# App will be available at http://localhost:8501

### Project Structure
autogen-agents/
â”œâ”€â”€ streamlit_app.py         # Minimal Streamlit app (Planner â†’ Assistant two-step)
â”œâ”€â”€ planning/                # Work dir for code generated by the Assistant
â”‚   â”œâ”€â”€ find_repo.py         # Example artifacts created during runs
â”‚   â””â”€â”€ fix_typo.py
â”œâ”€â”€ .env                     # Environment variables (create this)
â”œâ”€â”€ .env.example             # Template for .env
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml          # Dark theme for clean screenshots
â”œâ”€â”€ pyproject.toml           # Dependencies (streamlit, autogen-agentchat, etc.)

### Usage
Start the app.
Enter a task (e.g., â€œSuggest a fix to an open good first issue of flamlâ€).
Click â€œRun Multiâ€‘Agent Chat.â€
Watch the â€œConversation Logâ€:
Planner proposes a plan first.
Assistant then writes/executes Python code (with retries if needed).
See the final message and review any generated files under planning/.

### Troubleshooting
FLAML warning:
â€œflaml.automl is not availableâ€¦â€ is harmless for this app (weâ€™re not using AutoML). You can ignore it.
No response / no logs:
Ensure 
.env
 is loaded and GEMINI_API_KEY is set. Restart the app after changes.

Docker execution:
Set USE_DOCKER=true, have Docker Desktop running, and keep the docker Python package installed.
Environment Variables
GEMINI_API_KEY (required): Your Gemini key.
GEMINI_MODEL (optional): e.g., gemini-2.0-flash or gemini-1.5-pro.
GEMINI_BASE_URL (optional): Defaults to Googleâ€™s OpenAI-compatible endpoint.
TEMPERATURE (optional): Default 0.
MAX_AUTO_REPLIES (optional): Default 10.
USE_DOCKER (optional): true or false (default false).

### Contribution
Contributions are welcome. Please fork and open a PR with focused, learningâ€‘oriented improvements (e.g., small UI clarifications, better prompts, or optional tests).